{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e503ec4d-e97a-4ee0-9110-b8f17dca41e1",
   "metadata": {},
   "source": [
    "# Blend Anything\n",
    "\n",
    "Blend any two LoRA models. They should be from the same base model, but that is not strictly required.\n",
    "\n",
    "This uses a very small amount of VRAM (usually < 4GB for Flux/Qwen models) by streaming the layers and blending each one individually.\n",
    "\n",
    "This can resize the LoRAs while blending them. You can target any arbitrary rank, larger or smaller than the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a388cc9-992b-44f7-b5dc-9fde9887407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict, Iterable, Optional, Tuple, Set, Literal\n",
    "import torch\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c535125d-a1ed-4115-b6d4-b67a0b146d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_A = \".lora_A.weight\"\n",
    "_B = \".lora_B.weight\"\n",
    "\n",
    "# ---------- key pairing ----------\n",
    "def _pair_bases(sd: Dict[str, torch.Tensor]) -> Set[str]:\n",
    "    a = {k[:-len(_A)] for k in sd.keys() if k.endswith(_A)}\n",
    "    b = {k[:-len(_B)] for k in sd.keys() if k.endswith(_B)}\n",
    "    return a & b\n",
    "\n",
    "def _iter_bases_union(sd1: Dict[str, torch.Tensor],\n",
    "                      sd2: Optional[Dict[str, torch.Tensor]] = None,\n",
    "                      include_bases: Optional[Iterable[str]] = None) -> Iterable[str]:\n",
    "    bases = _pair_bases(sd1)\n",
    "    if sd2:\n",
    "        bases |= _pair_bases(sd2)\n",
    "    if include_bases is not None:\n",
    "        bases &= set(include_bases)\n",
    "    # sort for deterministic output\n",
    "    for base in tqdm(sorted(bases)):\n",
    "        yield base\n",
    "\n",
    "# ---------- math helpers ----------\n",
    "@torch.no_grad()\n",
    "def _delta_from_AB(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
    "    # A: [r, in], B: [out, r] -> Δ: [out, in]\n",
    "    return B @ A\n",
    "\n",
    "def _best_rank_for(deltaW: torch.Tensor, target_rank: Optional[int]) -> int:\n",
    "    m, n = deltaW.shape\n",
    "    max_r = min(m, n)\n",
    "    if target_rank is None:\n",
    "        return max_r\n",
    "    return max(1, min(int(target_rank), max_r))\n",
    "\n",
    "@torch.no_grad()\n",
    "def _truncated_factorization(\n",
    "    deltaW: torch.Tensor,\n",
    "    target_rank: int,\n",
    "    method: Literal[\"svd\", \"pca_lowrank\"] = \"svd\",\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Return (A', B') with shapes [r, in], [out, r] such that B'@A' ≈ Δ.\n",
    "    - 'svd': exact/truncated SVD (robust, a bit heavier)\n",
    "    - 'pca_lowrank': approximate (faster, lower memory on large mats)\n",
    "    \"\"\"\n",
    "    r = _best_rank_for(deltaW, target_rank)\n",
    "\n",
    "    if method == \"pca_lowrank\":\n",
    "        # Δ ≈ U S Vh via PCA low-rank approx\n",
    "        # q oversampling for better accuracy\n",
    "        U, S, V = torch.pca_lowrank(deltaW, q=r + 8, center=False)\n",
    "        # keep top-r\n",
    "        U = U[:, :r]\n",
    "        S = S[:r]\n",
    "        V = V[:, :r]\n",
    "        Vh = V.T\n",
    "    else:\n",
    "        # exact economy SVD then truncate\n",
    "        U, S, Vh = torch.linalg.svd(deltaW, full_matrices=False)\n",
    "        U = U[:, :r]\n",
    "        S = S[:r]\n",
    "        Vh = Vh[:r, :]\n",
    "\n",
    "    sroot = torch.sqrt(torch.clamp(S, min=0))\n",
    "    Bp = U * sroot.unsqueeze(0)      # [out, r]\n",
    "    Ap = sroot.unsqueeze(1) * Vh     # [r, in]\n",
    "    return Ap.contiguous(), Bp.contiguous()\n",
    "\n",
    "# ---------- device / dtype helpers ----------\n",
    "def _to_dev_dtype(x: torch.Tensor, device: Optional[torch.device], dtype: Optional[torch.dtype]) -> torch.Tensor:\n",
    "    if device is not None:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "    if dtype is not None and x.dtype != dtype:\n",
    "        x = x.to(dtype)\n",
    "    return x\n",
    "\n",
    "def _maybe_empty_cuda():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ---------- main: streaming blend & rank-convert ----------\n",
    "@torch.no_grad()\n",
    "def blend_and_convert_loras_streaming(\n",
    "    lora1: Dict[str, torch.Tensor],\n",
    "    lora2: Optional[Dict[str, torch.Tensor]] = None,\n",
    "    w1: float = 1.0,\n",
    "    w2: float = 1.0,\n",
    "    target_rank: Optional[int] = 32,\n",
    "    compute_device: Optional[str] = None,      # e.g. \"cuda:0\" or \"cpu\"\n",
    "    compute_dtype: Optional[torch.dtype] = torch.float32,\n",
    "    include_bases: Optional[Iterable[str]] = None,\n",
    "    drop_near_zero: bool = True,\n",
    "    zero_tol: float = 1e-12,\n",
    "    factor_method: Literal[\"svd\", \"pca_lowrank\"] = \"svd\",\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Streaming, low-memory LoRA blend/convert.\n",
    "    Processes one layer at a time; GPU RAM is bounded by one ΔW + SVD work.\n",
    "    Returns a CPU state_dict with .lora_A/B tensors of target rank.\n",
    "    \"\"\"\n",
    "    dev = torch.device(compute_device) if compute_device else None\n",
    "    out_sd: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "    for base in _iter_bases_union(lora1, lora2, include_bases):\n",
    "        # pull A/B from each lora if present\n",
    "        A1 = lora1.get(base + _A, None)\n",
    "        B1 = lora1.get(base + _B, None)\n",
    "        A2 = lora2.get(base + _A, None) if lora2 else None\n",
    "        B2 = lora2.get(base + _B, None) if lora2 else None\n",
    "\n",
    "        if A1 is None or B1 is None:\n",
    "            d1 = None\n",
    "        else:\n",
    "            A1d = _to_dev_dtype(A1, dev, compute_dtype)\n",
    "            B1d = _to_dev_dtype(B1, dev, compute_dtype)\n",
    "            d1 = _delta_from_AB(A1d, B1d)\n",
    "\n",
    "        if lora2 is not None and A2 is not None and B2 is not None:\n",
    "            A2d = _to_dev_dtype(A2, dev, compute_dtype)\n",
    "            B2d = _to_dev_dtype(B2, dev, compute_dtype)\n",
    "            d2 = _delta_from_AB(A2d, B2d)\n",
    "        else:\n",
    "            d2 = None\n",
    "\n",
    "        # blended Δ\n",
    "        if d1 is None and d2 is None:\n",
    "            _maybe_empty_cuda()\n",
    "            continue\n",
    "        elif d1 is None:\n",
    "            blended = w2 * d2\n",
    "        elif d2 is None:\n",
    "            blended = w1 * d1\n",
    "        else:\n",
    "            blended = w1 * d1 + w2 * d2\n",
    "\n",
    "        # optionally skip numerically tiny layers\n",
    "        if drop_near_zero and blended.abs().max().item() < zero_tol:\n",
    "            del blended, d1, d2\n",
    "            _maybe_empty_cuda()\n",
    "            continue\n",
    "\n",
    "        # factor to target rank (on compute device), then move to CPU\n",
    "        r = _best_rank_for(blended, target_rank)\n",
    "        A_new, B_new = _truncated_factorization(blended, r, method=factor_method)\n",
    "        A_new_cpu = A_new.detach().to(\"cpu\", copy=True)\n",
    "        B_new_cpu = B_new.detach().to(\"cpu\", copy=True)\n",
    "\n",
    "        # store\n",
    "        out_sd[base + _A] = A_new_cpu\n",
    "        out_sd[base + _B] = B_new_cpu\n",
    "\n",
    "        # hard cleanup for current layer before next\n",
    "        del A_new, B_new, A_new_cpu, B_new_cpu, blended, d1, d2\n",
    "        _maybe_empty_cuda()\n",
    "\n",
    "    return out_sd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eadc6049-3a6c-40b5-9aab-ae20e0e8bfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import safetensors.torch\n",
    "\n",
    "lora1 = safetensors.torch.load_file(\"/mnt/models/tensors/loras/qwen_image/nsfw_qwen_bs8_r32_lowlr_000005500.safetensors\")\n",
    "lora2 = safetensors.torch.load_file(\"/mnt/models/tensors/loras/qwen_image/nsfw_qwen_resume_detail_qha.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a7faa-4ce9-498a-b859-58f260fa25ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2888bbaf016342659fd53a60e50a9251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/840 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "blend16 = blend_and_convert_loras_streaming(\n",
    "    lora1, \n",
    "    lora2,\n",
    "    w1=0.5, \n",
    "    w2=0.75, \n",
    "    target_rank=16, \n",
    "    compute_device=\"cuda:0\", \n",
    "    compute_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "safetensors.torch.save_file(blend16, \"/mnt/models/tensors/loras/qwen_image/nsfw_qwen_blend_5500_qha.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cb7625-55f2-42e3-9575-2105ab75d036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
